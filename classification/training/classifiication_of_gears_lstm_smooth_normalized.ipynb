{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo\n",
    "- Test \n",
    "\n",
    "- Preprocessing/normalization/smoothing \n",
    "\n",
    "- Move functions into utils file (more functions)\n",
    "\n",
    "- Optimize hyperparameters \n",
    "\n",
    "- Clean up\n",
    "\n",
    "- Write metrics - average/std\n",
    "\n",
    "- Fix plots\n",
    "\n",
    "- Move from notebook to python files\n",
    "\n",
    "- Fix for all notebooks\n",
    "\n",
    "- Fix weight saving\n",
    "\n",
    "- Check TODOs in code\n",
    "\n",
    "- Clean files \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, DataLoader, Subset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from utils import train_and_validate, plot_single_metric, compute_mean_std, plot_train_val, plot_metric, CustomDataset, METRICS_NAMES, calc_avg_metrics, create_train_val_dataloaders\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train params\n",
    "train_size = 0.8\n",
    "val_size = 0.1\n",
    "test_size = 0.1\n",
    "batch_size = 16\n",
    "learning_rate = 0.0001\n",
    "epochs = 5\n",
    "patience = 100 # For early stopping\n",
    "k_folds = 3\n",
    "\n",
    "# check and select device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "#choosen_joints = [\"RAnkle_x\", \"LAnkle_x\"]\n",
    "choosen_joints = [\"LShoulder_x\",\"RShoulder_y\",\"LElbow_x\",\"RElbow_x\",\"LWrist_y\",\"RWrist_x\",\"LHip_y\",\"RHip_x\",\"LKnee_x\",\"RKnee_x\",\"LAnkle_x\", \"RAnkle_x\"]\n",
    "# \"LShoulder\",\"RShoulder\",\"LElbow\",\"RElbow\",\"LWrist\",\"RWrist\",\"LHip\",\"RHip\",\"LKnee\",\"RKnee\",\"LAnkle\",\"Head\",\"Neck\",\"LBigToe\",\"RBigToe\",\"LSmallToe\",\"RSmallToe\",\"LHeel\",\"RHeel\"\n",
    "\n",
    "# labels\n",
    "LABELS = {\n",
    "    \"unknown\": 0,\n",
    "    \"gear2\" : 1,\n",
    "    \"gear3\" : 2,\n",
    "    \"gear4\" : 3,\n",
    "}\n",
    "\n",
    "# file config\n",
    "files = [\"09\", \n",
    "         \"11\", \n",
    "         #\"13\",\n",
    "         \"14_cut\", \n",
    "         \"15_cut\", \n",
    "         #\"16_cut\",\n",
    "         \"17_cut\", \n",
    "         \"18_cut\", \n",
    "         \"19_cut\", \n",
    "         \"20_cut\", \n",
    "         \"21_cut\", \n",
    "         \"22_cut\", \n",
    "         \"23_cut\",\n",
    "         \"25\", \n",
    "         \"27\",\n",
    "         \"32\",\n",
    "         \"33\",\n",
    "         \"34\",\n",
    "         \"36\",\n",
    "         \"38\", \n",
    "         \"40\", \n",
    "         \"42\",\n",
    "         \"43\", \n",
    "         \"44\", \n",
    "         \"53\",\n",
    "         \"54_cut\"]\n",
    "file_name = \"labeled_cycles_\"\n",
    "path = \"../cycle_splits/labeled_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Train size = 217, Val size = 109\n",
      "Fold 2: Train size = 217, Val size = 109\n",
      "Fold 3: Train size = 218, Val size = 108\n"
     ]
    }
   ],
   "source": [
    "# def create_train_val_dataloaders(path, choosen_joints):\n",
    "#     # Create initial dataset (without normalization)\n",
    "#     train_dataset = CustomDataset(path, choosen_joints, padding_value=float('nan'), apply_gaussian_filter=False)\n",
    "\n",
    "#     # Split into Train+Val and Test\n",
    "#     generator = torch.Generator().manual_seed(42)\n",
    "#     dataset_size = len(train_dataset)\n",
    "#     train_val_size = int(dataset_size * (train_size + val_size))  # 90% for Train+Val\n",
    "#     test_size = dataset_size - train_val_size  # 10% for Test\n",
    "\n",
    "#     # Get indices for Train+Val and Test\n",
    "#     train_val_indices, test_indices = random_split(range(dataset_size), [train_val_size, test_size], generator=generator)\n",
    "\n",
    "#     # Initialize KFold (5 splits)\n",
    "#     kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "#     train_val_indices = list(train_val_indices)  # Convert to list\n",
    "#     fold_loaders = []\n",
    "\n",
    "#     for fold, (train_idx, val_idx) in enumerate(kf.split(train_val_indices)):\n",
    "#         print(f\"Fold {fold+1}: Train size = {len(train_idx)}, Val size = {len(val_idx)}\")\n",
    "        \n",
    "#         # Get actual dataset indices\n",
    "#         train_subset_indices = [train_val_indices[i] for i in train_idx]\n",
    "#         val_subset_indices = [train_val_indices[i] for i in val_idx]\n",
    "\n",
    "#         # Create Subsets (before normalization)\n",
    "#         train_subset = Subset(train_dataset, train_subset_indices)\n",
    "\n",
    "#         # Compute mean and std from the training subset\n",
    "#         all_train_samples = torch.cat([train_subset[i][0] for i in range(len(train_subset))], dim=0)\n",
    "#         mean = all_train_samples.nanmean(dim=0)\n",
    "#         std = torch.std(all_train_samples[~torch.isnan(all_train_samples)])\n",
    "\n",
    "#         # Create a new dataset with computed mean/std (normalize train and val using train stats)\n",
    "#         normalized_dataset = CustomDataset(path, choosen_joints, mean=mean, std=std, apply_gaussian_filter=True)\n",
    "\n",
    "#         # Create train/val subsets on normalized dataset\n",
    "#         train_data = Subset(normalized_dataset, train_subset_indices)\n",
    "#         val_data = Subset(normalized_dataset, val_subset_indices)\n",
    "\n",
    "#         # Create DataLoaders\n",
    "#         train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "#         val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#         # Store loaders for this fold\n",
    "#         fold_loaders.append((train_loader, val_loader))\n",
    "#     return fold_loaders\n",
    "\n",
    "fold_loaders = create_train_val_dataloaders(path, choosen_joints, train_size, val_size, k_folds, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_val_indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Select an index to compare (e.g., the first sample in the train split)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sample_idx \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_val_indices\u001b[49m[\u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# Ensure we use the same sample from both datasets\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Extract the raw signal (before normalization)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m raw_signal, raw_label \u001b[38;5;241m=\u001b[39m train_dataset[sample_idx]  \u001b[38;5;66;03m# From the first dataset instance\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_val_indices' is not defined"
     ]
    }
   ],
   "source": [
    "# Select an index to compare (e.g., the first sample in the train split)\n",
    "sample_idx = train_val_indices[2]  # Ensure we use the same sample from both datasets\n",
    "\n",
    "# Extract the raw signal (before normalization)\n",
    "raw_signal, raw_label = train_dataset[sample_idx]  # From the first dataset instance\n",
    "\n",
    "# Extract the processed signal (after normalization)\n",
    "normalized_signal, norm_label = normalized_dataset[sample_idx]  # From the second dataset instance\n",
    "\n",
    "# Ensure labels match\n",
    "assert raw_label == norm_label, \"Labels should match between datasets\"\n",
    "\n",
    "# Plot the signals for each joint\n",
    "print(raw_label)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i in range(raw_signal.shape[1]):  # Iterate over each joint\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(raw_signal[:, i], label=f'Joint {i}')\n",
    "    plt.title(\"Raw Signal (Before Normalization)\")\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(normalized_signal[:, i], label=f'Joint {i}')\n",
    "    plt.title(\"Normalized Signal (After Normalization)\")\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_size, output, num_layers, dropout):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_channels, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Output layer (fully connected layer after LSTM)\n",
    "        self.fc = nn.Linear(hidden_size, output)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state and cell state for LSTM\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        output, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        output = self.dropout(output[:, -1, :])\n",
    "        \n",
    "        output = self.fc(output)  # (batch_size, output_size)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = train_dataset[0][0].shape[1] # Input size is the amount of joints\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "output = len(set(train_dataset.labels))\n",
    "net = LSTMNet(input_channels=input_channels, \n",
    "            hidden_size=hidden_size, \n",
    "            output=output, \n",
    "            num_layers=num_layers, \n",
    "            dropout=dropout,\n",
    "            )\n",
    "net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeds = [42, 7, 13, 67, 89]\n",
    "seeds = [42,7]\n",
    "all_results = []\n",
    "best_train_cms = []\n",
    "best_val_cms = []\n",
    "\n",
    "for fold, (train_loader, val_loader) in enumerate(fold_loaders):\n",
    "    print(f\"\\n>>> Training on Fold {fold+1} <<<\\n\")\n",
    "    \n",
    "    # Initialize a new model for each fold\n",
    "    for seed in seeds:\n",
    "        print(f\"\\n========== Running for Seed {seed} on Fold {fold+1} ==========\\n\")\n",
    "        \n",
    "        # Set seed for reproducibility\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "        net = LSTMNet(input_channels=input_channels, \n",
    "                      hidden_size=hidden_size, \n",
    "                      output=output, \n",
    "                      num_layers=num_layers, \n",
    "                      dropout=dropout)\n",
    "        net.to(device)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Train and validate\n",
    "        results, best_train_cm, best_val_cm = train_and_validate(seed, \n",
    "                                                                 net, \n",
    "                                                                 criterion, \n",
    "                                                                 optimizer,\n",
    "                                                                 epochs,\n",
    "                                                                 learning_rate,\n",
    "                                                                 patience,\n",
    "                                                                 train_loader,\n",
    "                                                                 val_loader,\n",
    "                                                                 device\n",
    "                                                                 )\n",
    "        \n",
    "        # Store results\n",
    "        all_results.append({'seed': seed, 'fold': fold+1, 'results': results})\n",
    "        best_train_cms.append({'seed': seed, 'fold': fold+1, 'cm': best_train_cm})\n",
    "        best_val_cms.append({'seed': seed, 'fold': fold+1, 'cm': best_val_cm})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('runs/cross_validation_experiment_' + datetime.now().strftime(\"%Y_%m_%d_%H_%M\") ) # Format as HH:MM:SS)\n",
    "\n",
    "final_results = calc_avg_metrics(k_folds, all_results, seeds, epochs)\n",
    "\n",
    "def write_cv_results(fold_final_results, epochs, writer):\n",
    "    \n",
    "    # Loop through each fold\n",
    "    for fold in fold_final_results:\n",
    "        for epoch in range(epochs):  # Loop through 5 epochs\n",
    "            # Log train metrics\n",
    "            writer.add_scalar(f'Fold_{fold}/Train/Loss', fold_final_results[fold]['train_losses'][epoch], epoch)\n",
    "            writer.add_scalar(f'Fold_{fold}/Train/Accuracy', fold_final_results[fold]['train_accs'][epoch], epoch)\n",
    "            writer.add_scalar(f'Fold_{fold}/Train/Precision', fold_final_results[fold]['train_precisions'][epoch], epoch)\n",
    "            writer.add_scalar(f'Fold_{fold}/Train/Recall', fold_final_results[fold]['train_recalls'][epoch], epoch)\n",
    "            writer.add_scalar(f'Fold_{fold}/Train/F1', fold_final_results[fold]['train_f1s'][epoch], epoch)\n",
    "            \n",
    "            # Log validation metrics\n",
    "            writer.add_scalar(f'Fold_{fold}/Val/Loss', fold_final_results[fold]['val_losses'][epoch], epoch)\n",
    "            writer.add_scalar(f'Fold_{fold}/Val/Accuracy', fold_final_results[fold]['val_accs'][epoch], epoch)\n",
    "            writer.add_scalar(f'Fold_{fold}/Val/Precision', fold_final_results[fold]['val_precisions'][epoch], epoch)\n",
    "            writer.add_scalar(f'Fold_{fold}/Val/Recall', fold_final_results[fold]['val_recalls'][epoch], epoch)\n",
    "            writer.add_scalar(f'Fold_{fold}/Val/F1', fold_final_results[fold]['val_f1s'][epoch], epoch)\n",
    "\n",
    "write_cv_results(final_results, epochs, writer)     \n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT base code for plots\n",
    "# Extract metrics\n",
    "metrics = [\"losses\", \"accs\", \"precisions\", \"recalls\", \"f1s\"]\n",
    "mean_std_results = {metric: {\n",
    "    \"train\": compute_mean_std(all_results, f\"train_{metric}\"),\n",
    "    \"val\": compute_mean_std(all_results, f\"val_{metric}\")\n",
    "} for metric in metrics}\n",
    "\n",
    "# Plot combined graphs\n",
    "plot_train_val(all_results, \"losses\", \"Train vs Validation Loss\", \"Loss\", mean_std_results)\n",
    "plot_train_val(all_results, \"accs\", \"Train vs Validation Accuracy\", \"Accuracy (%)\", mean_std_results)\n",
    "plot_train_val(all_results, \"f1s\", \"Train vs Validation F1-Score\", \"F1 Score\", mean_std_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = len(all_results[0][\"train_losses\"])  # Assuming all have same number of epochs\n",
    "x = np.arange(1, epochs + 1)  # X-axis values (Epochs)\n",
    "metrics = [\"train_losses\", \"train_accs\", \"train_precisions\", \"train_recalls\", \"train_f1s\",\n",
    "           \"val_losses\", \"val_accs\", \"val_precisions\", \"val_recalls\", \"val_f1s\"]\n",
    "mean_std_results = {metric: compute_mean_std(all_results, metric) for metric in metrics}\n",
    "\n",
    "# Plot training loss\n",
    "plot_metric(\"train_losses\", \"Training Loss Over Epochs\", x, \"Loss\", mean_std_results)\n",
    "\n",
    "# Plot validation loss\n",
    "plot_metric(\"val_losses\", \"Validation Loss Over Epochs\", x, \"Loss\", mean_std_results)\n",
    "\n",
    "# Plot accuracy\n",
    "plot_metric(\"train_accs\", \"Training Accuracy Over Epochs\", x, \"Accuracy (%)\", mean_std_results)\n",
    "plot_metric(\"val_accs\", \"Validation Accuracy Over Epochs\", x, \"Accuracy (%)\", mean_std_results)\n",
    "\n",
    "# Plot F1-score\n",
    "plot_metric(\"train_f1s\", \"Training F1-Score Over Epochs\", x, \"F1 Score\", mean_std_results)\n",
    "plot_metric(\"val_f1s\", \"Validation F1-Score Over Epochs\", x, \"F1 Score\", mean_std_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, results in enumerate(all_results):    \n",
    "    epochs_trained = len(results[\"train_losses\"])\n",
    "    epoch_range = range(1, epochs_trained + 1)\n",
    "\n",
    "    print(\"_\"*100)\n",
    "    print(f\"Results for seed {seeds[i]}\")\n",
    "\n",
    "    plt.figure(figsize=(15, 12))\n",
    "\n",
    "    # Plotting training and validation loss\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plot_single_metric(epoch_range, results[\"train_losses\"], results[\"val_losses\"], 'Loss', 'Epoch', 'Loss')\n",
    "\n",
    "    # Plotting training and validation accuracy\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plot_single_metric(epoch_range, results[\"train_accs\"], results[\"val_accs\"], 'Accuracy', 'Epoch', 'Accuracy (%)')\n",
    "\n",
    "    # Plotting training and validation precision\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plot_single_metric(epoch_range, results[\"train_precisions\"], results[\"val_precisions\"], 'Precision', 'Epoch', 'Precision')\n",
    "\n",
    "    # Plotting training and validation recall\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plot_single_metric(epoch_range, results[\"train_recalls\"], results[\"val_recalls\"], 'Recall', 'Epoch', 'Recall')\n",
    "\n",
    "    # Plotting training and validation F1 scores\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plot_single_metric(epoch_range, results[\"train_f1s\"], results[\"val_f1s\"], 'F1 Score', 'Epoch', 'F1 Score')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    disp_train_cm = ConfusionMatrixDisplay(best_train_cms[i], display_labels=list(LABELS.keys())[:len(set(dataset.labels))])\n",
    "    disp_train_cm.plot()\n",
    "    plt.title(f\"Seed {seeds[i]} training lowest val loss confusion matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    disp_val_cm = ConfusionMatrixDisplay(best_val_cms[i], display_labels=list(LABELS.keys())[:len(set(dataset.labels))])\n",
    "    disp_val_cm.plot()\n",
    "    plt.title(f\"Seed {seeds[i]} validation lowest val loss confusion matrix\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors for different seeds\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']  # Add more if needed\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Metric names and corresponding keys in the results dictionary\n",
    "metrics = {\n",
    "    \"Loss\": (\"train_losses\", \"val_losses\"),\n",
    "    \"Accuracy\": (\"train_accs\", \"val_accs\"),\n",
    "    \"Precision\": (\"train_precisions\", \"val_precisions\"),\n",
    "    \"Recall\": (\"train_recalls\", \"val_recalls\"),\n",
    "    \"F1 Score\": (\"train_f1s\", \"val_f1s\"),\n",
    "}\n",
    "\n",
    "# Iterate over each metric and create a subplot for it\n",
    "for idx, (metric_name, (train_key, val_key)) in enumerate(metrics.items(), 1):\n",
    "    plt.subplot(2, 3, idx)  # Arrange plots in a 2x3 grid\n",
    "    \n",
    "    for i, results in enumerate(all_results):\n",
    "        epochs_trained = len(results[train_key])\n",
    "        epoch_range = range(1, epochs_trained + 1)\n",
    "        color = colors[i % len(colors)]  # Assign a color\n",
    "\n",
    "        # Plot training metric\n",
    "        plt.plot(epoch_range, results[train_key], label=f'Training Seed {seeds[i]}', linestyle='--', color=color)\n",
    "        # Plot validation metric\n",
    "        plt.plot(epoch_range, results[val_key], label=f'Validation Seed {seeds[i]}', linestyle='-', color=color)\n",
    "\n",
    "    plt.title(f'Training & Validation {metric_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors for different seeds\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']  # Add more if needed\n",
    "\n",
    "plt.figure(figsize=(15, 20))  # Larger figure to fit 10 subplots\n",
    "\n",
    "# Metric names and corresponding keys in the results dictionary\n",
    "metrics = {\n",
    "    \"Loss\": (\"train_losses\", \"val_losses\"),\n",
    "    \"Accuracy\": (\"train_accs\", \"val_accs\"),\n",
    "    \"Precision\": (\"train_precisions\", \"val_precisions\"),\n",
    "    \"Recall\": (\"train_recalls\", \"val_recalls\"),\n",
    "    \"F1 Score\": (\"train_f1s\", \"val_f1s\"),\n",
    "}\n",
    "\n",
    "# Iterate over each metric and create subplots\n",
    "for idx, (metric_name, (train_key, val_key)) in enumerate(metrics.items()):\n",
    "    \n",
    "    # Training Plot\n",
    "    plt.subplot(5, 2, 2 * idx + 1)  # Left column for training\n",
    "    for i, results in enumerate(all_results):\n",
    "        epochs_trained = len(results[train_key])\n",
    "        epoch_range = range(1, epochs_trained + 1)\n",
    "        color = colors[i % len(colors)]\n",
    "        \n",
    "        plt.plot(epoch_range, results[train_key], label=f'Seed {seeds[i]}', color=color)\n",
    "    \n",
    "    plt.title(f'Training {metric_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "\n",
    "    # Validation Plot\n",
    "    plt.subplot(5, 2, 2 * idx + 2)  # Right column for validation\n",
    "    for i, results in enumerate(all_results):\n",
    "        epochs_trained = len(results[val_key])\n",
    "        epoch_range = range(1, epochs_trained + 1)\n",
    "        color = colors[i % len(colors)]\n",
    "        \n",
    "        plt.plot(epoch_range, results[val_key], label=f'Seed {seeds[i]}', color=color)\n",
    "    \n",
    "    plt.title(f'Validation {metric_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alphapose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
